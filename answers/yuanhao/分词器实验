# åˆ›å»ºæ–‡ä»¶ï¼šbpe_final_train.py
import collections
import re
import random
from tqdm import tqdm
import time
import pickle
import os

print("=== BPE åˆ†è¯å™¨è®­ç»ƒ ===")


class BPETokenizer:
    def __init__(self):
        self.merges = {}
        self.vocab = set()

    def get_stats(self, vocab):
        """è®¡ç®—ç›¸é‚»ç¬¦å·å¯¹çš„é¢‘ç‡"""
        pairs = collections.defaultdict(int)
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i + 1]] += freq
        return pairs

    def merge_vocab(self, pair, v_in):
        """åˆå¹¶æœ€é¢‘ç¹çš„ç¬¦å·å¯¹"""
        v_out = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        pattern = re.compile(r'(?<!\S)' + re.escape(bigram) + r'(?!\S)')

        for word in v_in:
            w_out = pattern.sub(replacement, word)
            v_out[w_out] = v_in[word]
        return v_out

    def train(self, text, vocab_size=10000, show_progress=True):
        """è®­ç»ƒ BPE åˆ†è¯å™¨"""
        print("é¢„å¤„ç†æ–‡æœ¬...")
        start_preprocess = time.time()

        # è½¬æ¢ä¸ºå°å†™å¹¶åˆ†è¯
        text = text.lower()
        words = text.split()

        # åˆå§‹åŒ–è¯æ±‡è¡¨
        vocab = collections.Counter([f"{' '.join(list(word))} </w>" for word in words])
        initial_vocab_size = len(set(' '.join(vocab.keys()).split()))

        preprocess_time = time.time() - start_preprocess

        print(f"å•è¯æ•°é‡: {len(words):,}")
        print(f"åˆå§‹è¯æ±‡è¡¨å¤§å°: {initial_vocab_size}")
        print(f"é¢„å¤„ç†æ—¶é—´: {preprocess_time:.2f} ç§’")

        self.merges = {}
        current_vocab_size = initial_vocab_size

        print(f"\nå¼€å§‹ BPE è®­ç»ƒï¼Œç›®æ ‡è¯æ±‡é‡: {vocab_size}")
        start_time = time.time()

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        pbar = tqdm(total=vocab_size - initial_vocab_size,
                    desc="è®­ç»ƒè¿›åº¦",
                    unit="merge",
                    ncols=100) if show_progress else None

        merge_count = 0
        while current_vocab_size < vocab_size:
            pairs = self.get_stats(vocab)
            if not pairs:
                print("\næ²¡æœ‰æ›´å¤šå¯åˆå¹¶çš„å¯¹")
                break

            best_pair = max(pairs, key=pairs.get)
            best_freq = pairs[best_pair]

            if best_freq < 2:
                print("\næœ€å¸¸è§å¯¹é¢‘ç‡ < 2")
                break

            # æ‰§è¡Œåˆå¹¶
            vocab = self.merge_vocab(best_pair, vocab)
            self.merges[best_pair] = best_pair[0] + best_pair[1]
            merge_count += 1

            # æ›´æ–°è¯æ±‡è¡¨å¤§å°
            current_vocab = set()
            for word in vocab.keys():
                current_vocab.update(word.split())
            current_vocab_size = len(current_vocab)

            if pbar:
                pbar.update(1)
                pbar.set_postfix({
                    'è¯æ±‡é‡': current_vocab_size,
                    'åˆå¹¶': f"{best_pair[0]}{best_pair[1]}",
                    'é¢‘ç‡': best_freq
                })

            # æ¯ 500 æ¬¡åˆå¹¶æ˜¾ç¤ºä¸€æ¬¡ä¿¡æ¯
            if merge_count % 500 == 0:
                elapsed = time.time() - start_time
                print(f"\n[æ£€æŸ¥ç‚¹] åˆå¹¶ {merge_count}, è¯æ±‡é‡ {current_vocab_size}, ç”¨æ—¶ {elapsed:.1f}s")

        if pbar:
            pbar.close()

        # æ„å»ºæœ€ç»ˆè¯æ±‡è¡¨
        self.vocab = set()
        for word in vocab.keys():
            self.vocab.update(word.split())

        training_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
        print(f"ğŸ”„ åˆå¹¶æ¬¡æ•°: {len(self.merges)}")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        print(f"ğŸš€ è®­ç»ƒé€Ÿåº¦: {merge_count / training_time:.1f} åˆå¹¶/ç§’")

        return self

    def encode_word(self, word):
        """ç¼–ç å•ä¸ªå•è¯"""
        tokens = list(word) + ['</w>']

        changed = True
        while changed and len(tokens) > 1:
            changed = False
            # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåˆå¹¶é•¿çš„token
            sorted_merges = sorted(self.merges.items(),
                                   key=lambda x: -len(x[0][0] + x[0][1]))

            for (pair, merge_result) in sorted_merges:
                i = 0
                while i < len(tokens) - 1:
                    if tokens[i] == pair[0] and tokens[i + 1] == pair[1]:
                        tokens = tokens[:i] + [merge_result] + tokens[i + 2:]
                        changed = True
                    else:
                        i += 1
                if changed:
                    break
        return tokens

    def encode(self, text):
        """ç¼–ç æ–‡æœ¬"""
        words = text.lower().split()
        tokens = []
        for word in words:
            tokens.extend(self.encode_word(word))
        return tokens

    def decode(self, tokens):
        """è§£ç  tokens"""
        text = ''.join(tokens)
        text = text.replace('</w>', ' ')
        return text.strip()

    def analyze_vocab(self):
        """åˆ†æè¯æ±‡è¡¨"""
        vocab_sizes = {}
        for token in self.vocab:
            length = len(token)
            vocab_sizes[length] = vocab_sizes.get(length, 0) + 1

        print(f"\nğŸ“ˆ è¯æ±‡è¡¨åˆ†æ:")
        for length in sorted(vocab_sizes.keys()):
            count = vocab_sizes[length]
            percentage = (count / len(self.vocab)) * 100
            print(f"  é•¿åº¦ {length}: {count:4d} tokens ({percentage:5.1f}%)")

        # æ˜¾ç¤ºä¸€äº›å¸¸è§çš„å¤šå­—ç¬¦token
        multi_char = [t for t in self.vocab if len(t) > 1 and t != '</w>']
        common_tokens = sorted(multi_char, key=len, reverse=True)[:15]
        print(f"  ğŸ”¤ å¸¸è§å¤šå­—ç¬¦token: {common_tokens}")


def load_data():
    """åŠ è½½æ•°æ®"""
    file_path = "/root/autodl-tmp/TinyStoriesV2-GPT4-valid.txt"

    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        print("è¯·å…ˆå®Œæˆä¸‹è½½")
        return None

    print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  - æ€»å­—ç¬¦æ•°: {len(content):,}")
    print(f"  - æ€»è¡Œæ•°: {content.count(chr(10)) + 1:,}")

    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    lines = content.split('\n')
    print(f"  - æ•°æ®é¢„è§ˆ:")
    for i in range(min(3, len(lines))):
        print(f"    {lines[i][:80]}...")

    return content


def sample_documents(text, num_docs=10):
    """æŠ½æ ·æ–‡æ¡£"""
    documents = text.split('\n\n')
    documents = [doc.strip() for doc in documents if len(doc.strip()) > 100]

    print(f"\nğŸ“„ æ–‡æ¡£ç»Ÿè®¡:")
    print(f"  - æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    print(f"  - å¹³å‡æ–‡æ¡£é•¿åº¦: {sum(len(doc) for doc in documents) / len(documents):.0f} å­—ç¬¦")

    sampled = random.sample(documents, min(num_docs, len(documents)))
    print(f"  - æŠ½æ · {len(sampled)} ä¸ªæ–‡æ¡£ç”¨äºæµ‹è¯•")

    return sampled


def calculate_compression(tokenizer, documents):
    """è®¡ç®—å‹ç¼©ç‡"""
    print(f"\nğŸ§® è®¡ç®—å‹ç¼©ç‡...")
    results = []

    for i, doc in enumerate(documents):
        original_chars = len(doc)
        tokens = tokenizer.encode(doc)
        num_tokens = len(tokens)
        compression_ratio = original_chars / num_tokens

        print(f"  æ–‡æ¡£ {i + 1:2d}: {original_chars:5d} å­—ç¬¦ â†’ {num_tokens:4d} tokens "
              f"(å‹ç¼©ç‡: {compression_ratio:.2f} å­—èŠ‚/token)")

        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç¼–ç ç¤ºä¾‹
        if i == 0:
            print(f"     ç¼–ç ç¤ºä¾‹: {tokens[:20]}...")

        results.append({
            'original_chars': original_chars,
            'num_tokens': num_tokens,
            'compression_ratio': compression_ratio
        })

    return results


def main():
    # åŠ è½½æ•°æ®
    text = load_data()
    if text is None:
        return

    # æŠ½æ ·æ–‡æ¡£
    documents = sample_documents(text, 10)

    # è®­ç»ƒåˆ†è¯å™¨
    print(f"\nğŸ”§ å¼€å§‹è®­ç»ƒ 10K è¯æ±‡é‡çš„ BPE åˆ†è¯å™¨...")
    tokenizer = BPETokenizer()
    tokenizer.train(text, vocab_size=10000)

    # åˆ†æè¯æ±‡è¡¨
    tokenizer.analyze_vocab()

    # è®¡ç®—å‹ç¼©ç‡
    results = calculate_compression(tokenizer, documents)

    # ç»Ÿè®¡åˆ†æ
    total_chars = sum(r['original_chars'] for r in results)
    total_tokens = sum(r['num_tokens'] for r in results)
    avg_compression = total_chars / total_tokens

    print(f"\nğŸ“ˆ === æœ€ç»ˆç»“æœ ===")
    print(f"  ğŸ“ æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"  ğŸ”¤ æ€» Token æ•°: {total_tokens:,}")
    print(f"  ğŸ“¦ å¹³å‡å‹ç¼©ç‡: {avg_compression:.2f} å­—èŠ‚/token")
    print(f"  ğŸ“‰ Token å‡å°‘æ¯”ä¾‹: {(1 - total_tokens / total_chars) * 100:.1f}%")

    # ä¿å­˜æ¨¡å‹
    model_path = 'bpe_10k_tinystories.pkl'
    with open(model_path, 'wb') as f:
        pickle.dump({
            'merges': tokenizer.merges,
            'vocab': tokenizer.vocab,
            'compression_ratio': avg_compression,
            'vocab_size': len(tokenizer.vocab)
        }, f)
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º '{model_path}'")

    # æµ‹è¯•ç¼–è§£ç 
    print(f"\nğŸ” ç¼–è§£ç æµ‹è¯•:")
    test_text = "The quick brown fox jumps over the lazy dog."
    tokens = tokenizer.encode(test_text)
    decoded = tokenizer.decode(tokens)
    print(f"  åŸæ–‡: '{test_text}'")
    print(f"  Tokens: {tokens}")
    print(f"  è§£ç : '{decoded}'")
    print(f"  âœ“ ç¼–è§£ç ä¸€è‡´: {decoded == test_text.lower()}")


if __name__ == "__main__":
    main()
