class Tokenizer:
    def __init__(self, vocab, merges, special_tokens=None):
        self.vocab = vocab  # id -> bytes
        self.merges = merges
        self.special_tokens = special_tokens or []

        # 创建反向映射
        self.vocab_inv = {v: k for k, v in vocab.items()}
        self.PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

    @classmethod
    def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None):
        # 从文件加载 vocab 和 merges
        # 实现文件加载逻辑
        pass

    def encode(self, text: str) -> List[int]:
        """编码文本为 token IDs"""
        # 1. 预分词
        chunks = re.split(f'({"|".join(re.escape(t) for t in self.special_tokens)})', text)

        tokens = []
        for chunk in chunks:
            if chunk in self.special_tokens:
                # 特殊 token
                tokens.append(self.vocab_inv[chunk.encode('utf-8')])
            else:
                # 普通文本，应用 BPE
                words = re.findall(self.PAT, chunk)
                for word in words:
                    word_bytes = list(word.encode('utf-8'))
                    word_tokens = [bytes([b]) for b in word_bytes]

                    # 应用合并规则
                    for merge in self.merges:
                        new_word_tokens = []
                        i = 0
                        while i < len(word_tokens):
                            if (i < len(word_tokens) - 1 and
                                    word_tokens[i] == merge[0] and
                                    word_tokens[i + 1] == merge[1]):
                                new_word_tokens.append(merge[0] + merge[1])
                                i += 2
                            else:
                                new_word_tokens.append(word_tokens[i])
                                i += 1
                        word_tokens = new_word_tokens

                    # 转换为 IDs
                    for token in word_tokens:
                        tokens.append(self.vocab_inv[token])

        return tokens

    def decode(self, ids: List[int]) -> str:
        """解码 token IDs 为文本"""
        byte_sequence = b''
        for token_id in ids:
            byte_sequence += self.vocab[token_id]

        return byte_sequence.decode('utf-8', errors='replace')
