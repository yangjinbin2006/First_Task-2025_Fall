# problem_3_4_fixed.py
import re
import collections
from typing import Dict, List, Tuple
import time
import psutil
import os
import cProfile
import pstats


class BPETrainer:
    def __init__(self):
        # ä½¿ç”¨å…¼å®¹çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ \p{} è¯­æ³•ï¼‰
        self.PAT = r"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-Z]+| ?[0-9]+| ?[^\sa-zA-Z0-9]+|\s+(?!\S)|\s+"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå­—æ¯ã€æ•°å­—ã€éå­—æ¯æ•°å­—å­—ç¬¦ã€ç©ºæ ¼

    def train_bpe(self, input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[
        Dict[int, bytes], List[Tuple[bytes, bytes]]]:
        """
        Problem 3: è®­ç»ƒ BPE åˆ†è¯å™¨
        """
        print("=== Problem 3: BPE åˆ†è¯å™¨è®­ç»ƒ ===")
        start_time = time.time()
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        # 1. è¯»å–æ•°æ®
        print("1. è¯»å–è®­ç»ƒæ•°æ®...")
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                text = f.read()
            print(f"   âœ“ è¯»å–æˆåŠŸ: {len(text):,} å­—ç¬¦")
        except FileNotFoundError:
            print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return {}, []

        # 2. åˆå§‹åŒ–è¯æ±‡è¡¨
        print("2. åˆå§‹åŒ–è¯æ±‡è¡¨...")
        vocab = {}
        merges = []
        token_id = 0

        # æ·»åŠ ç‰¹æ®Š tokens
        for special in special_tokens:
            vocab[token_id] = special.encode('utf-8')
            print(f"   âœ“ æ·»åŠ ç‰¹æ®Štoken: {special} -> ID {token_id}")
            token_id += 1

        # æ·»åŠ åŸºç¡€å­—èŠ‚ (0-255)
        base_bytes_count = 0
        for byte_val in range(256):
            vocab[token_id] = bytes([byte_val])
            token_id += 1
            base_bytes_count += 1

        print(f"   âœ“ æ·»åŠ åŸºç¡€å­—èŠ‚: {base_bytes_count} ä¸ªå­—èŠ‚ (ID {len(special_tokens)}-{token_id - 1})")

        # 3. é¢„åˆ†è¯
        print("3. è¿›è¡Œé¢„åˆ†è¯...")

        # æŒ‰ç‰¹æ®Š token åˆ†å‰²æ–‡æœ¬
        if special_tokens:
            pattern = '|'.join(re.escape(token) for token in special_tokens)
            chunks = re.split(f'({pattern})', text)
        else:
            chunks = [text]

        # æ”¶é›†æ‰€æœ‰å•è¯åŠå…¶é¢‘ç‡
        word_freq = collections.Counter()
        special_token_count = 0

        for chunk in chunks:
            if chunk in special_tokens:
                special_token_count += 1
                continue  # è·³è¿‡ç‰¹æ®Štoken

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯
            try:
                words = re.findall(self.PAT, chunk)
                word_freq.update(words)
            except Exception as e:
                print(f"   âš  åˆ†è¯é”™è¯¯: {e}")
                # ç®€å•çš„å›é€€åˆ†è¯
                words = chunk.split()
                word_freq.update(words)

        print(f"   âœ“ æ‰¾åˆ° {len(word_freq):,} ä¸ªå”¯ä¸€å•è¯")
        print(f"   âœ“ æ‰¾åˆ° {special_token_count} ä¸ªç‰¹æ®Štoken")
        if word_freq:
            print(f"   âœ“ æœ€å¸¸è§çš„5ä¸ªå•è¯: {word_freq.most_common(5)}")

        if not word_freq:
            print("   âš  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å•è¯ï¼Œä½¿ç”¨ç®€å•åˆ†è¯...")
            words = text.split()
            word_freq.update(words)
            print(f"   âœ“ ä½¿ç”¨ç®€å•åˆ†è¯æ‰¾åˆ° {len(word_freq)} ä¸ªå•è¯")

        # 4. å°†å•è¯è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—
        print("4. å‡†å¤‡å­—èŠ‚åºåˆ—...")
        byte_words = {}
        total_tokens_initial = 0

        for word, freq in word_freq.items():
            try:
                byte_sequence = list(word.encode('utf-8'))
                byte_tokens = [bytes([b]) for b in byte_sequence]
                byte_words[word] = (byte_tokens, freq)
                total_tokens_initial += len(byte_tokens) * freq
            except Exception as e:
                print(f"   âš  è·³è¿‡å•è¯ '{word}': {e}")
                continue

        print(f"   âœ“ åˆå§‹æ€»tokenæ•°: {total_tokens_initial:,}")
        print(f"   âœ“ æœ‰æ•ˆå•è¯æ•°: {len(byte_words)}")

        if not byte_words:
            print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„å•è¯å¯ä»¥è®­ç»ƒ")
            return vocab, merges

        # 5. BPE åˆå¹¶è®­ç»ƒ
        print("5. å¼€å§‹ BPE åˆå¹¶è®­ç»ƒ...")
        print(f"   â†’ ç›®æ ‡è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   â†’ å½“å‰è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print(f"   â†’ éœ€è¦åˆå¹¶æ¬¡æ•°: {vocab_size - len(vocab)}")

        merge_count = 0
        max_merges = vocab_size - len(vocab)

        while len(vocab) < vocab_size and merge_count < max_merges:
            # ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡
            pair_freq = collections.defaultdict(int)
            for word_data in byte_words.values():
                tokens, freq = word_data
                for i in range(len(tokens) - 1):
                    pair = (tokens[i], tokens[i + 1])
                    pair_freq[pair] += freq

            if not pair_freq:
                print("   âš  æ²¡æœ‰æ›´å¤šå¯åˆå¹¶çš„å­—èŠ‚å¯¹")
                break

            # æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹ï¼ˆå¤„ç†å¹³å±€ï¼šé¢‘ç‡ç›¸åŒé€‰å­—å…¸åºæ›´å¤§çš„ï¼‰
            best_pair = max(pair_freq.items(), key=lambda x: (x[1], x[0]))[0]
            best_freq = pair_freq[best_pair]

            if best_freq < 2:
                print(f"   âš  æœ€é¢‘ç¹å¯¹é¢‘ç‡ < 2ï¼Œåœæ­¢è®­ç»ƒ")
                break

            # åˆ›å»ºæ–° token
            new_token = best_pair[0] + best_pair[1]
            vocab[token_id] = new_token
            merges.append(best_pair)

            # è¿›åº¦æ˜¾ç¤º
            if (merge_count + 1) % 50 == 0 or (merge_count + 1) <= 10:
                progress = (merge_count + 1) / max_merges * 100
                try:
                    pair1_str = best_pair[0].decode('utf-8', errors='replace')
                    pair2_str = best_pair[1].decode('utf-8', errors='replace')
                    new_str = new_token.decode('utf-8', errors='replace')
                    print(f"   â†³ åˆå¹¶ {merge_count + 1:4d}/{max_merges} ({progress:5.1f}%): "
                          f"'{pair1_str}' + '{pair2_str}' â†’ '{new_str}' (é¢‘ç‡: {best_freq})")
                except:
                    print(f"   â†³ åˆå¹¶ {merge_count + 1:4d}/{max_merges} ({progress:5.1f}%): "
                          f"{best_pair[0]} + {best_pair[1]} â†’ {new_token} (é¢‘ç‡: {best_freq})")

            # æ›´æ–°æ‰€æœ‰å•è¯ä¸­çš„å­—èŠ‚å¯¹
            new_byte_words = {}
            for word, (tokens, freq) in byte_words.items():
                new_tokens = []
                i = 0
                while i < len(tokens):
                    if (i < len(tokens) - 1 and
                            tokens[i] == best_pair[0] and
                            tokens[i + 1] == best_pair[1]):
                        new_tokens.append(new_token)
                        i += 2
                    else:
                        new_tokens.append(tokens[i])
                        i += 1
                new_byte_words[word] = (new_tokens, freq)

            byte_words = new_byte_words
            token_id += 1
            merge_count += 1

        # 6. è®­ç»ƒå®Œæˆç»Ÿè®¡
        training_time = time.time() - start_time
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_usage = final_memory - initial_memory

        print(f"\nğŸ‰ BPE è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        print(f"   ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
        print(f"   ğŸ“š æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print(f"   ğŸ”„ åˆå¹¶è§„åˆ™æ•°é‡: {len(merges)}")
        print(f"   ğŸ“ å¤„ç†çš„å•è¯æ•°é‡: {len(byte_words)}")

        # åˆ†æè¯æ±‡è¡¨
        self._analyze_vocabulary(vocab)

        return vocab, merges

    def _analyze_vocabulary(self, vocab: Dict[int, bytes]):
        """åˆ†æè¯æ±‡è¡¨"""
        print(f"\nğŸ“ˆ è¯æ±‡è¡¨åˆ†æ:")

        if not vocab:
            print("   è¯æ±‡è¡¨ä¸ºç©º")
            return

        # æŒ‰tokené•¿åº¦åˆ†ç»„
        length_groups = {}
        for token_bytes in vocab.values():
            length = len(token_bytes)
            if length not in length_groups:
                length_groups[length] = []
            length_groups[length].append(token_bytes)

        print(f"   Tokené•¿åº¦åˆ†å¸ƒ:")
        for length in sorted(length_groups.keys()):
            tokens = length_groups[length]
            percentage = (len(tokens) / len(vocab)) * 100
            print(f"     é•¿åº¦ {length}: {len(tokens):4d} tokens ({percentage:5.1f}%)")

        # æ‰¾åˆ°æœ€é•¿çš„token
        longest_token = max(vocab.values(), key=len)
        max_length = len(longest_token)

        # æ‰¾åˆ°æ‰€æœ‰æœ€é•¿çš„token
        longest_tokens = [token for token in vocab.values() if len(token) == max_length]

        print(f"   æœ€é•¿çš„token (é•¿åº¦ {max_length}):")
        for i, token_bytes in enumerate(longest_tokens[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
            try:
                decoded = token_bytes.decode('utf-8', errors='replace')
                print(f"     {i + 1}. {token_bytes} -> '{decoded}'")
            except:
                print(f"     {i + 1}. {token_bytes} -> [æ— æ³•è§£ç ]")

        # æ˜¾ç¤ºä¸€äº›æœ‰æ„ä¹‰çš„åˆå¹¶
        print(f"\n   æœ‰æ„ä¹‰çš„åˆå¹¶ç¤ºä¾‹:")
        meaningful_tokens = []
        for token_bytes in vocab.values():
            if len(token_bytes) > 1:
                try:
                    decoded = token_bytes.decode('utf-8', errors='replace')
                    if any(c.isalpha() for c in decoded) and len(decoded) > 1:
                        meaningful_tokens.append((token_bytes, decoded))
                    if len(meaningful_tokens) >= 10:
                        break
                except:
                    pass

        for token_bytes, decoded in meaningful_tokens:
            print(f"     {token_bytes} -> '{decoded}'")


def run_train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[
    Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """
    Problem 3: BPEè®­ç»ƒé€‚é…å™¨å‡½æ•°
    """
    trainer = BPETrainer()
    return trainer.train_bpe(input_path, vocab_size, special_tokens)


# Problem 4 çš„å…·ä½“å®ç°
def problem_4_tinystories():
    """
    Problem 4: åœ¨ TinyStories ä¸Šè®­ç»ƒ BPE
    """
    print("\n" + "=" * 60)
    print("PROBLEM 4: åœ¨ TinyStories ä¸Šè®­ç»ƒ BPE")
    print("=" * 60)

    # é…ç½®å‚æ•°
    input_path = "TinyStoriesV2-GPT4-valid.txt"  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
    vocab_size = 5000
    special_tokens = ["<|endoftext|>"]

    print(f"(a) è®­ç»ƒé…ç½®:")
    print(f"   æ•°æ®é›†: {input_path}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"   ç‰¹æ®Štoken: {special_tokens}")

    if not os.path.exists(input_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        print("è¯·ä¸‹è½½ TinyStoriesV2-GPT4-valid.txt åˆ°å½“å‰ç›®å½•")
        return None, None

    # æ€§èƒ½åˆ†æ
    print(f"\n(b) å¼€å§‹æ€§èƒ½åˆ†æ...")

    def train_with_profiling():
        trainer = BPETrainer()
        return trainer.train_bpe(input_path, vocab_size, special_tokens)

    # ä½¿ç”¨ cProfile è¿›è¡Œæ€§èƒ½åˆ†æ
    profiler = cProfile.Profile()
    profiler.enable()

    start_time = time.time()
    vocab, merges = train_with_profiling()
    total_time = time.time() - start_time

    profiler.disable()

    # è¾“å‡ºæ€§èƒ½åˆ†æç»“æœ
    print(f"\nğŸ“Š æ€§èƒ½åˆ†æç»“æœ (æœ€è€—æ—¶çš„10ä¸ªå‡½æ•°):")
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)

    # Problem 4 ç­”æ¡ˆæ€»ç»“
    print(f"\n" + "=" * 60)
    print("PROBLEM 4 ç­”æ¡ˆæ€»ç»“:")
    print("=" * 60)
    print(f"(a) è®­ç»ƒç»“æœ:")
    print(f"   - è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
    print(f"   - å†…å­˜å ç”¨: è§ä¸Šæ–¹è®­ç»ƒç»Ÿè®¡")

    if vocab:
        longest_token = max(vocab.values(), key=len)
        try:
            longest_decoded = longest_token.decode('utf-8', errors='replace')
            print(f"   - æœ€é•¿token: {longest_token} -> '{longest_decoded}' (é•¿åº¦: {len(longest_token)} å­—èŠ‚)")
        except:
            print(f"   - æœ€é•¿token: {longest_token} (é•¿åº¦: {len(longest_token)} å­—èŠ‚)")

    print(f"\n(b) æ€§èƒ½åˆ†æ:")
    print(f"   - æœ€è€—æ—¶çš„éƒ¨åˆ†é€šå¸¸æ˜¯: å­—èŠ‚å¯¹é¢‘ç‡ç»Ÿè®¡å’Œåˆå¹¶æ“ä½œ")
    print(f"   - å»ºè®®ä¼˜åŒ–: ä½¿ç”¨å¢é‡æ›´æ–°è€Œä¸æ˜¯é‡æ–°è®¡ç®—é¢‘ç‡")
    print(f"   - é¢„åˆ†è¯é˜¶æ®µä¹Ÿå¯èƒ½æˆä¸ºç“¶é¢ˆ")

    return vocab, merges


if __name__ == "__main__":
    # æµ‹è¯• Problem 3
    print("æµ‹è¯• Problem 3 çš„å°è§„æ¨¡è®­ç»ƒ...")

    # åˆ›å»ºæµ‹è¯•æ•°æ® (ä½¿ç”¨ä½œä¸šä¸­çš„ç¤ºä¾‹)
    test_data = """low low low low low
lower lower 
widest widest widest
newest newest newest newest newest newest"""

    with open("test_data.txt", "w", encoding="utf-8") as f:
        f.write(test_data)

    # è¿è¡Œå°è§„æ¨¡è®­ç»ƒæµ‹è¯•
    print("ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®­ç»ƒ...")
    test_vocab, test_merges = run_train_bpe(
        input_path="test_data.txt",
        vocab_size=50,  # å°è¯æ±‡è¡¨ç”¨äºæµ‹è¯•
        special_tokens=["<|endoftext|>"]
    )

    print(f"\næµ‹è¯•è®­ç»ƒç»“æœ:")
    print(f"è¯æ±‡è¡¨å¤§å°: {len(test_vocab)}")
    print(f"åˆå¹¶è§„åˆ™æ•°é‡: {len(test_merges)}")
    if test_merges:
        print(f"å‰5ä¸ªåˆå¹¶è§„åˆ™:")
        for i, merge in enumerate(test_merges[:5]):
            try:
                pair1 = merge[0].decode('utf-8', errors='replace')
                pair2 = merge[1].decode('utf-8', errors='replace')
                print(f"  {i + 1}. {merge[0]} + {merge[1]} -> '{pair1}{pair2}'")
            except:
                print(f"  {i + 1}. {merge[0]} + {merge[1]}")

    # è¿è¡Œ Problem 4 (å¦‚æœæ•°æ®æ–‡ä»¶å­˜åœ¨)
    print("\n" + "=" * 60)
    if os.path.exists("TinyStoriesV2-GPT4-valid.txt"):
        problem_4_tinystories()
    else:
        print(f"æœªæ‰¾åˆ° TinyStories æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡ Problem 4")
        print(f"è¯·å°† TinyStoriesV2-GPT4-valid.txt æ”¾åœ¨å½“å‰ç›®å½•")
